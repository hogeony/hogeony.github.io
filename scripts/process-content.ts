import fs from 'fs';
import path from 'path';
import matter from 'gray-matter';
import { getAllContentTypes } from '../src/lib/content';

/**
 * ì½˜í…ì¸  íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³  LLMì„ í†µí•´ ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
 * 
 * ì‚¬ìš©ë²•:
 * npm run process-content [type] [slug]
 * 
 * ì˜ˆì‹œ:
 * npm run process-content diary 2024-01-15-my-diary
 */

async function processContent(type: string, slug: string) {
  const contentDirectory = path.join(process.cwd(), 'content');
  const filePath = path.join(contentDirectory, type, `${slug}.md`);

  if (!fs.existsSync(filePath)) {
    console.error(`âŒ File not found: ${filePath}`);
    process.exit(1);
  }

  const fileContents = fs.readFileSync(filePath, 'utf8');
  const { data, content } = matter(fileContents);

  console.log(`ğŸ“„ Processing: ${type}/${slug}`);
  console.log(`ğŸ“ Title: ${data.title || 'No title'}`);

  // LLM API í˜¸ì¶œ (ì„ íƒì‚¬í•­)
  // ì°¸ê³ : ì •ì  export ëª¨ë“œì—ì„œëŠ” API Routesë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ
  // ì§ì ‘ OpenAI APIë¥¼ í˜¸ì¶œí•˜ê±°ë‚˜ ë³„ë„ ì„œë²„ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
  if (process.env.OPENAI_API_KEY) {
    try {
      const { default: OpenAI } = await import('openai');
      const openai = new OpenAI({
        apiKey: process.env.OPENAI_API_KEY,
      });

      const prompt = `ë‹¤ìŒ ì½˜í…ì¸ ë¥¼ ë¶„ì„í•˜ì—¬ ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

íƒ€ì…: ${type}
ì œëª©: ${data.title || ''}
ë‚´ìš©:
${content.substring(0, 2000)}...

ë‹¤ìŒ í˜•ì‹ì˜ JSONì„ ë°˜í™˜í•´ì£¼ì„¸ìš”:
{
  "tldr": "2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½",
  "tags": ["íƒœê·¸1", "íƒœê·¸2", "íƒœê·¸3"],
  "sentiment": "positive | neutral | negative",
  "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"]
}`;

      const completion = await openai.chat.completions.create({
        model: 'gpt-4',
        messages: [
          {
            role: 'system',
            content:
              'You are a helpful assistant that analyzes content and generates metadata in Korean.',
          },
          {
            role: 'user',
            content: prompt,
          },
        ],
        temperature: 0.7,
        max_tokens: 500,
      });

      const responseText = completion.choices[0]?.message?.content || '{}';
      
      // JSON íŒŒì‹± ì‹œë„
      let metadata;
      try {
        const jsonMatch = responseText.match(/```json\n([\s\S]*?)\n```/) || 
                         responseText.match(/```\n([\s\S]*?)\n```/) ||
                         [null, responseText];
        metadata = JSON.parse(jsonMatch[1] || responseText);
      } catch (e) {
        metadata = {
          tldr: content.substring(0, 150) + '...',
          tags: [],
          sentiment: 'neutral',
          keywords: [],
        };
      }

      // Front matter ì—…ë°ì´íŠ¸
      const updatedData = {
        ...data,
        tldr: metadata.tldr || data.tldr,
        tags: metadata.tags || data.tags || [],
        meta: {
          ...data.meta,
          ...metadata,
          generated: new Date().toISOString(),
          llm_model: 'gpt-4',
        },
      };

      // íŒŒì¼ ì €ì¥
      const updatedContent = matter.stringify(content, updatedData);
      fs.writeFileSync(filePath, updatedContent);
      console.log(`âœ… File updated with LLM metadata: ${filePath}`);
    } catch (error) {
      console.warn('âš ï¸  LLM API error:', error);
    }
  } else {
    console.warn('âš ï¸  OPENAI_API_KEY not set, skipping LLM processing');
  }
}

// CLI ì¸ì íŒŒì‹±
const args = process.argv.slice(2);
if (args.length < 2) {
  console.log('Usage: npm run process-content [type] [slug]');
  console.log('Example: npm run process-content diary 2024-01-15-my-diary');
  process.exit(1);
}

const [type, slug] = args;

if (!getAllContentTypes().includes(type as any)) {
  console.error(`âŒ Invalid type: ${type}`);
  console.error(`Valid types: ${getAllContentTypes().join(', ')}`);
  process.exit(1);
}

processContent(type, slug).catch((error) => {
  console.error('âŒ Error:', error);
  process.exit(1);
});
